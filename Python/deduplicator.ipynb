{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b96f57-6de8-42ad-b2e2-3fac25001bb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# THE GREAT FILE DE-DUPLICATOR\n",
    "### A solution to my fractured backups\n",
    "A program of 1000 lines begins with dependency imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3dd947-fe45-4994-b5a2-da49210349c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import threading\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pprint import pprint, pformat\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97978f-baa3-4efe-9cd3-9021e6a3e295",
   "metadata": {},
   "source": [
    "### Enter your parameters\n",
    "Enter where you want to output your analysis files, what directories you want to analyze for duplicates and which folder names you wish to exclude from the analysis.\n",
    "Mind the backslashes playing nice with python on windows.\n",
    "\n",
    "**THE ORDER THE deDupeDirs ARE IN ARE THE PREFERRED ORDER FOR RETENTION IF DRY RUN IS DISABLED! MAKE SURE YOU ARE OK WITH LATER ENTRIES IN THE LIST HAVING DUPLICATE FILES DISABLED!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e952a8d5-8bd0-48aa-9ffd-f59bb1353e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C:\\\\Users\\\\Ryan\\\\Desktop', 'G:\\\\', 'F:\\\\')\n"
     ]
    }
   ],
   "source": [
    "dedupeDirs = (r'C:\\Users\\Ryan\\Desktop',\n",
    "               #r'D:\\Users\\Ryan',\n",
    "               'G:\\\\',\n",
    "               'F:\\\\',\n",
    "               #'H:\\\\',\n",
    "               )\n",
    "excludeDirs = {'$RECYCLE.BIN',\n",
    "               'gdpr data',\n",
    "               'steamapps',\n",
    "               'Google Dump',\n",
    "              }\n",
    "excludeExtensions = {'.example',    \n",
    "}\n",
    "outputDir = Path(r'C:\\Users\\Ryan\\Desktop')\n",
    "\n",
    "#Assign file retention priority to dedupeDirs based on order\n",
    "#Early entries in the list will keep their files over later entries with duplicate files\n",
    "print(dedupeDirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e79f80-50d1-4bc7-894d-655c624f0447",
   "metadata": {},
   "source": [
    "## Find duplicates based on file name\n",
    "For a given directory listed in \"deDupeDirs\", get the filenames, last modified dates, and file size. \n",
    "End result of a list will be a dict with filename as keys and values that are lists of dicts of file paths and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c21d6-1711-4f63-b196-f7084dd4c9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running in C:\\Users\\Ryan\\Desktop\n"
     ]
    }
   ],
   "source": [
    "dupeSearchStart = time.time()\n",
    "dedupePaths = [Path(x) for x in dedupeDirs]\n",
    "\n",
    "nameSet = set()\n",
    "files = []\n",
    "dupeDict = defaultdict(list)\n",
    "for wd in dedupePaths:\n",
    "    print(f'running in {wd}')\n",
    "    dirDupeSearchStart = time.time()\n",
    "    for file in wd.glob('**/*'):\n",
    "        if file.is_file() and not excludeDirs.intersection(set(file.parts)) and not excludeExtensions.intersection(set(file.suffixes)):\n",
    "            fileDict = {'PathObj':file,\n",
    "                        'path':file.as_posix(),\n",
    "                        'size':file.stat().st_size,\n",
    "                        'mtime':file.stat().st_mtime,\n",
    "                        }\n",
    "            try:\n",
    "                dupeDict[file.name].append(fileDict)\n",
    "            except:\n",
    "                logging.exception(f\"couldn't add this filedict, continuing \\n {fileDict}\")\n",
    "                raise\n",
    "    print('len of files parsed = ', len(dupeDict))\n",
    "    print(f'processed {wd} in {time.time() - dirDupeSearchStart}s')\n",
    "\n",
    "uniqueDict = {k:v for k,v in dupeDict.items() if len(v) == 1}\n",
    "print('number of uniques = ', len(uniqueDict))\n",
    "dupeDict   = {k:v for k,v in dupeDict.items() if len(v) > 1}\n",
    "print('number of dupes = ', len(dupeDict))\n",
    "print(f'dict load completed in {time.time() - dupeSearchStart}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e3d85-44b9-46be-9899-c6d5619f8bd9",
   "metadata": {},
   "source": [
    "### Save your progress\n",
    "The above steps could take a while depending on how much you've hoarded, save the output to a JSON file to noodle with at a later date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed87fd8-098b-47f3-90d3-2c6122b02535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to file\n",
      "done, took 2.026458501815796s\n"
     ]
    }
   ],
   "source": [
    "writingStart = time.time()\n",
    "print('writing to file')    \n",
    "with open(outputDir.joinpath('duplicateFiles.json'),'w',encoding='utf8') as f:\n",
    "    json.dump(dupeDict, f, indent = 1 )\n",
    "    \n",
    "with open(outputDir.joinpath('uniqueFiles.json'),'w',encoding='utf8') as f:\n",
    "    json.dump(uniqueDict, f, indent = 1 )\n",
    "print(f'done, took {time.time() - writingStart}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ef5e7-b5a4-4932-b8cf-17c4d9f3f553",
   "metadata": {},
   "source": [
    "### Reload your progress\n",
    "Reload if you saved to a JSON file.  You'll need to run imports and the parameter code blocks before this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4fe08d08-a76c-4bf3-a8ed-8779560dfe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from file\n",
      "done, took 0.21904969215393066s\n"
     ]
    }
   ],
   "source": [
    "loadStart = time.time()\n",
    "print('loading from file')    \n",
    "with open(outputDir.joinpath('duplicateFiles.json'),'r',encoding='utf8') as f:\n",
    "    dupeDict = json.load(f)\n",
    "    \n",
    "with open(outputDir.joinpath('uniqueFiles.json'),'r',encoding='utf8') as f:\n",
    "    uniqueDict = json.load(f)\n",
    "print(f'done, took {time.time() - loadStart}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbff30-ad4d-4402-898e-4db707af568a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### start by figuring out duplication categories\n",
    " - same name, same size, same mtime, same hash different dedupeDirs\n",
    "     - can safely delete one of them based on priority\n",
    " - same name, same size, different mtime\n",
    "     - hash and branch logic if different\n",
    " - same name, different size \n",
    "     - send to manual review queue, potentially prefer the newer one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d8c651f7-4833-464b-91af-3093b951e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'mtime': 1607128670.8325891,\n",
      "  'path': 'C:/Users/Ryan/Desktop/desktop.ini',\n",
      "  'priority': 0,\n",
      "  'rootTier': 0,\n",
      "  'size': 282},\n",
      " {'mtime': 1538450652.6097116,\n",
      "  'path': 'C:/Users/Ryan/Desktop/Files/Home and Money/Tax/2015 Tax/desktop.ini',\n",
      "  'priority': 1,\n",
      "  'rootTier': 0,\n",
      "  'size': 136},\n",
      " {'mtime': 1577863147.0110621,\n",
      "  'path': 'C:/Users/Ryan/Desktop/Files/Music/desktop.ini',\n",
      "  'priority': 2,\n",
      "  'rootTier': 0,\n",
      "  'size': 440},\n",
      " {'mtime': 1538450652.605711,\n",
      "  'path': 'C:/Users/Ryan/Desktop/Files/photos/Old Photos/Trivia '\n",
      "          'Pics/desktop.ini',\n",
      "  'priority': 3,\n",
      "  'rootTier': 0,\n",
      "  'size': 136},\n",
      " {'mtime': 1616551386.9991388,\n",
      "  'path': 'C:/Users/Ryan/Desktop/Files/Work/work git/SQL/desktop.ini',\n",
      "  'priority': 4,\n",
      "  'rootTier': 0,\n",
      "  'size': 110},\n",
      " {'mtime': 1538450652.6097116,\n",
      "  'path': 'G:/Home and Money/Tax/2015 Tax/desktop.ini',\n",
      "  'priority': 5,\n",
      "  'rootTier': 1,\n",
      "  'size': 136},\n",
      " {'mtime': 1587357803.6098547,\n",
      "  'path': 'G:/Music/desktop.ini',\n",
      "  'priority': 6,\n",
      "  'rootTier': 1,\n",
      "  'size': 440},\n",
      " {'mtime': 1538450652.605711,\n",
      "  'path': 'G:/photos/Old Photos/Trivia Pics/desktop.ini',\n",
      "  'priority': 7,\n",
      "  'rootTier': 1,\n",
      "  'size': 136},\n",
      " {'mtime': 1538450652.6097116,\n",
      "  'path': 'F:/Files/Home and Money/Tax/2015 Tax/desktop.ini',\n",
      "  'priority': 8,\n",
      "  'rootTier': 2,\n",
      "  'size': 136},\n",
      " {'mtime': 1577863147.0110621,\n",
      "  'path': 'F:/Files/Music/desktop.ini',\n",
      "  'priority': 9,\n",
      "  'rootTier': 2,\n",
      "  'size': 440},\n",
      " {'mtime': 1538450652.605711,\n",
      "  'path': 'F:/Files/photos/Old Photos/Trivia Pics/desktop.ini',\n",
      "  'priority': 10,\n",
      "  'rootTier': 2,\n",
      "  'size': 136},\n",
      " {'mtime': 1579135968.026018,\n",
      "  'path': 'F:/Oculus Apps/desktop.ini',\n",
      "  'priority': 11,\n",
      "  'rootTier': 2,\n",
      "  'size': 127}]\n"
     ]
    }
   ],
   "source": [
    "if type(dedupeDirs[0]) == str:\n",
    "    dedupeDirs = [(rootTier,Path(directory).as_posix()) for rootTier,directory in enumerate(dedupeDirs)]\n",
    "\n",
    "for k,dupeList in dupeDict.items(): \n",
    "    for cnt, file in enumerate(dupeList):\n",
    "        #assign a tier to each identified dupe based on the directory order given in the first user parameters\n",
    "        for rootTier, dedupeDir in dedupeDirs:\n",
    "            if file['path'].startswith(dedupeDir):\n",
    "                file['rootTier'] = rootTier\n",
    "    dupeList = sorted(dupeList, key=lambda k: (k['rootTier'], len('path')))\n",
    "    for cnt,file in enumerate(dupeList):\n",
    "        file['priority'] = cnt \n",
    "    pprint(dupeList)\n",
    "\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9a2a0e-4a19-4ae8-bd8a-fd8e355406f7",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4abe84f-1560-4d63-8806-84292be32762",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupeList = sorted(dupeList, key=lambda k: (k['rootTier'], len('path')))\n",
    "for cnt, file in enumerate(dupeList):\n",
    "    file['priority'] = cnt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1afc1-c159-407a-9df9-dd68a9787ad9",
   "metadata": {},
   "source": [
    "### Let's get deleting files\n",
    "Work out the priority of each file to conslidate everything on one disk first.\n",
    "\n",
    "Dry run is enabled unless you disable it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fefaee5f-f53f-4614-84cd-5f51fecef86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bd04f41-3251-475e-b016-0a814eaedfda",
   "metadata": {},
   "source": [
    "dryRun = True\n",
    "\n",
    "## TODO:\n",
    " - compare by file attributes, basically just size and name\n",
    "     -  then by hash if same name/size\n",
    " - if name/size disagree, put in a conflict csv to review later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e38b1-d315-45a5-96ff-d36bd1a0975b",
   "metadata": {},
   "source": [
    "\n",
    "Bold \t**bold text**\n",
    "*italicized text*\n",
    "\n",
    "Blockquote \t> blockquote\n",
    "Ordered List \t\n",
    "1. First item\n",
    "2. Second item\n",
    "3. Third item\n",
    "Unordered List \t\n",
    "- First item\n",
    "- Second item\n",
    "- Third item\n",
    "Code \t`code`\n",
    "Horizontal Rule \t---\n",
    "Link \t\n",
    "[title](https://www.example.com)\n",
    "Image \t\n",
    "![alt text](image.jpg)\n",
    "\n",
    "### Results \n",
    "| Stretch/Untouched | ProbDistribution | Accuracy |\n",
    "| :- | -: | :-: |\n",
    "| Stretched | Gaussian | .843\n",
    "\n",
    " ```\n",
    "{\n",
    "  \"firstName\": \"John\",\n",
    "  \"lastName\": \"Smith\",\n",
    "  \"age\": 25\n",
    "}\n",
    "``` \n",
    "footnote \tHere's a sentence with a footnote. [^1]\n",
    "\n",
    "[^1]: This is the footnote.\n",
    "Heading ID \n",
    "### My Great Heading {#custom-id}\n",
    "Definition List \t\n",
    "term\n",
    ": definition\n",
    "\n",
    "Task List \t\n",
    "- [x] Write the press release\n",
    "- [ ] Update the website\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
