{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b96f57-6de8-42ad-b2e2-3fac25001bb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# THE GREAT FILE DE-DUPLICATOR\n",
    "### A solution to my fractured backups\n",
    "A program of 1000 lines begins with dependency imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a3dd947-fe45-4994-b5a2-da49210349c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from multiprocessing import Process, Queue\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pprint import pprint, pformat\n",
    "import time\n",
    "import logging\n",
    "#import pandas as pd\n",
    "import zlib\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97978f-baa3-4efe-9cd3-9021e6a3e295",
   "metadata": {},
   "source": [
    "### Enter your parameters\n",
    "Enter where you want to output your analysis files, what directories you want to analyze for duplicates and which folder names you wish to exclude from the analysis.\n",
    "Mind the backslashes playing nice with python on windows.\n",
    "\n",
    "**THE ORDER THE deDupeDirs ARE IN ARE THE PREFERRED ORDER FOR RETENTION IF DRY RUN IS DISABLED! MAKE SURE YOU ARE OK WITH LATER ENTRIES IN THE LIST HAVING DUPLICATE FILES DISABLED!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e952a8d5-8bd0-48aa-9ffd-f59bb1353e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C:\\\\Users\\\\Ryan\\\\Desktop', 'G:\\\\', 'F:\\\\')\n"
     ]
    }
   ],
   "source": [
    "dedupeDirs = (r'C:\\Users\\Ryan\\Desktop',\n",
    "               #r'D:\\Users\\Ryan',\n",
    "               'G:\\\\',\n",
    "               'F:\\\\',\n",
    "               #'H:\\\\',\n",
    "               )\n",
    "excludeDirs = {'$RECYCLE.BIN',\n",
    "               'gdpr data',\n",
    "               'steamapps',\n",
    "               'Google Dump',\n",
    "              }\n",
    "excludeExtensions = {'.example',    \n",
    "}\n",
    "outputDir = Path(r'C:\\Users\\Ryan\\Desktop')\n",
    "\n",
    "#Assign file retention priority to dedupeDirs based on order\n",
    "#Early entries in the list will keep their files over later entries with duplicate files\n",
    "print(dedupeDirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e79f80-50d1-4bc7-894d-655c624f0447",
   "metadata": {},
   "source": [
    "## Find duplicates based on file name\n",
    "For a given directory listed in \"deDupeDirs\", get the filenames, last modified dates, and file size. \n",
    "End result of a list will be a dict with filename as keys and values that are lists of dicts of file paths and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658c21d6-1711-4f63-b196-f7084dd4c9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running in C:\\Users\\Ryan\\Desktop\n",
      "len of files parsed =  41562\n",
      "processed C:\\Users\\Ryan\\Desktop in 23.478837728500366s\n",
      "running in G:\\\n",
      "len of files parsed =  41562\n",
      "processed G:\\ in 0.0s\n",
      "running in F:\\\n",
      "len of files parsed =  46694\n",
      "processed F:\\ in 66.96567034721375s\n",
      "number of uniques =  29437\n",
      "number of dupes =  17257\n",
      "dict load completed in 90.46351528167725s\n"
     ]
    }
   ],
   "source": [
    "dupeSearchStart = time.time()\n",
    "dedupePaths = [Path(x) for x in dedupeDirs]\n",
    "\n",
    "nameSet = set()\n",
    "files = []\n",
    "dupeDict = defaultdict(list)\n",
    "for wd in dedupePaths:\n",
    "    print(f'running in {wd}')\n",
    "    dirDupeSearchStart = time.time()\n",
    "    for file in wd.glob('**/*'):\n",
    "        if file.is_file() and not excludeDirs.intersection(set(file.parts)) and not excludeExtensions.intersection(set(file.suffixes)):\n",
    "            fileDict = {'pathObj':file,\n",
    "                        'path':file.as_posix(),\n",
    "                        'size':file.stat().st_size,\n",
    "                        'mtime':file.stat().st_mtime,\n",
    "                        }\n",
    "            try:\n",
    "                dupeDict[file.name].append(fileDict)\n",
    "            except:\n",
    "                logging.exception(f\"couldn't add this filedict, continuing \\n {fileDict}\")\n",
    "                raise\n",
    "    print('len of files parsed = ', len(dupeDict))\n",
    "    print(f'processed {wd} in {time.time() - dirDupeSearchStart}s')\n",
    "\n",
    "uniqueDict = {k:v for k,v in dupeDict.items() if len(v) == 1}\n",
    "print('number of uniques = ', len(uniqueDict))\n",
    "dupeDict   = {k:v for k,v in dupeDict.items() if len(v) > 1}\n",
    "print('number of dupes = ', len(dupeDict))\n",
    "print(f'dict load completed in {time.time() - dupeSearchStart}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e3d85-44b9-46be-9899-c6d5619f8bd9",
   "metadata": {},
   "source": [
    "### Save your progress\n",
    "The above steps could take a while depending on how much you've hoarded, save the output to a JSON file to noodle with at a later date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed87fd8-098b-47f3-90d3-2c6122b02535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to file\n",
      "done, took 1.00s\n"
     ]
    }
   ],
   "source": [
    "writingStart = time.time()\n",
    "print('writing to file')    \n",
    "dicts = (dupeDict, uniqueDict)\n",
    "for d in dicts:\n",
    "    for k,li in d.items():\n",
    "        for file in li:\n",
    "            del file['pathObj']\n",
    "            \n",
    "with open(outputDir.joinpath('duplicateFiles.json'),'w',encoding='utf8') as f:\n",
    "    json.dump(dupeDict, f, indent = 1 )\n",
    "    \n",
    "with open(outputDir.joinpath('uniqueFiles.json'),'w',encoding='utf8') as f:\n",
    "    json.dump(uniqueDict, f, indent = 1 )\n",
    "print(f'done, took {time.time() - writingStart:.2f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ef5e7-b5a4-4932-b8cf-17c4d9f3f553",
   "metadata": {},
   "source": [
    "### Reload your progress\n",
    "Reload if you saved to a JSON file.  You'll need to run imports and the parameter code blocks before this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fe08d08-a76c-4bf3-a8ed-8779560dfe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from file\n",
      "done, took 0.89s\n"
     ]
    }
   ],
   "source": [
    "loadStart = time.time()\n",
    "print('loading from file')    \n",
    "with open(outputDir.joinpath('duplicateFiles.json'),'r',encoding='utf8') as f:\n",
    "    dupeDict = json.load(f)\n",
    "    \n",
    "with open(outputDir.joinpath('uniqueFiles.json'),'r',encoding='utf8') as f:\n",
    "    uniqueDict = json.load(f)\n",
    "    \n",
    "dicts = (dupeDict, uniqueDict)\n",
    "for d in dicts:\n",
    "    for k,li in d.items():\n",
    "        for file in li:\n",
    "            file['pathObj'] = Path(file['path'])\n",
    "            \n",
    "print(f'done, took {(time.time() - loadStart):.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b193bb-3ac2-4d6e-9d78-9a2791d54847",
   "metadata": {},
   "source": [
    "### Only run on exact duplicates first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca46886-2137-4dfc-b23b-53cd0b829086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dupeGroupCount=0 of lenDupeGroups=17257 for a % complete of 0.00\n",
      "defaultdict(<class 'list'>,\n",
      "            {110: ['C:/Users/Ryan/Desktop/Files/Work/work git/SQL/desktop.ini'],\n",
      "             127: ['F:/Oculus Apps/desktop.ini'],\n",
      "             136: ['C:/Users/Ryan/Desktop/Files/Home and Money/Tax/2015 '\n",
      "                   'Tax/desktop.ini',\n",
      "                   'C:/Users/Ryan/Desktop/Files/photos/Old Photos/Trivia '\n",
      "                   'Pics/desktop.ini',\n",
      "                   'F:/Files/Home and Money/Tax/2015 Tax/desktop.ini',\n",
      "                   'F:/Files/photos/Old Photos/Trivia Pics/desktop.ini'],\n",
      "             282: ['C:/Users/Ryan/Desktop/desktop.ini'],\n",
      "             440: ['C:/Users/Ryan/Desktop/Files/Music/desktop.ini',\n",
      "                   'F:/Files/Music/desktop.ini']})\n",
      "{136: ['C:/Users/Ryan/Desktop/Files/Home and Money/Tax/2015 Tax/desktop.ini',\n",
      "       'C:/Users/Ryan/Desktop/Files/photos/Old Photos/Trivia Pics/desktop.ini',\n",
      "       'F:/Files/Home and Money/Tax/2015 Tax/desktop.ini',\n",
      "       'F:/Files/photos/Old Photos/Trivia Pics/desktop.ini'],\n",
      " 440: ['C:/Users/Ryan/Desktop/Files/Music/desktop.ini',\n",
      "       'F:/Files/Music/desktop.ini']}\n"
     ]
    }
   ],
   "source": [
    "def crc32(fileName):\n",
    "    with open(fileName, 'rb') as fh:\n",
    "        hash = 0\n",
    "        while True:\n",
    "            s = fh.read(65536)\n",
    "            if not s:\n",
    "                break\n",
    "            hash = zlib.crc32(s, hash)\n",
    "        return \"%08X\" % (hash & 0xFFFFFFFF)\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return f\"{num:3.1f}{unit}{suffix}\" \n",
    "        num /= 1024.0\n",
    "    return \"{num:.1f}Yi{suffix}\"\n",
    "\n",
    "\n",
    "    \n",
    "#input('test')\n",
    "progressTick = 20\n",
    "fileIncrement = 1\n",
    "maxProcesses = 8\n",
    "runningProcesses = []\n",
    "Q = Queue()\n",
    "crcStart = time.time()\n",
    "lenDupeGroups = len(dupeDict.values())\n",
    "progressTick = lenDupeGroups % progressTick\n",
    "print(f'starting {lenDupeGroups=}')\n",
    "\n",
    "\n",
    "for dupeGroupCount, kv in enumerate(dupeDict.items()):\n",
    "    filename,dupeList = kv\n",
    "    clear_output()\n",
    "    #if dupeGroupCount % fileIncrement == 0:\n",
    "    print(f'processing {dupeGroupCount=} of {lenDupeGroups=} for a % complete of {(dupeGroupCount/lenDupeGroups):.2f}')\n",
    "\n",
    "    #only things in sizeGroup get a CRC\n",
    "    sizeGroup = defaultdict(list)\n",
    "    for cnt,d in enumerate(dupeList):\n",
    "        sizeGroup[d['size']].append(d['path'])\n",
    "    #remove all where we don't have a duplicate  filesize\n",
    "    pprint(sizeGroup)\n",
    "    sizeGroup = {k:v for k,v in sizeGroup.items() if len(v) > 1}\n",
    "    pprint(sizeGroup)\n",
    "    input()\n",
    "    \n",
    "    \n",
    "    for cnt,d in enumerate(dupeList):\n",
    "        while len(runningProcesses) == maxProcesses:\n",
    "            time.sleep(1)\n",
    "        print(f\"processing {d['path']} {sizeof_fmt(d['size'])}\")\n",
    "        d['crc32'] = crc32(d['path'])\n",
    "    \n",
    "    \n",
    "    process = multiprocessing.Process(target=crc32, args =(d['path']))\n",
    "    runningProcesses.append(process)\n",
    "    process.start()\n",
    "\n",
    "    for p in runningProcesses:\n",
    "        p.join()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def my_func(arg):\n",
    "        Q.put('Hello, ' + arg)\n",
    "\n",
    "    p1 = Process(target=my_func, args=('John',))\n",
    "    p1.start()\n",
    "    print(Q.get())\n",
    "    p1.join()\n",
    "        \n",
    "    #fileSizeMatch = Counter([d['size']])\n",
    "    crcCount = Counter([d['crc32'] for d in dupeList])\n",
    "\n",
    "    for d in dupeList:\n",
    "        d['crcDupeCount'] = crcCount[d['crc32']]\n",
    "    \n",
    "    dupeList.sort(key=lambda x: x['crcDupeCount'], reverse=True)\n",
    "    #pprint(dupeList)\n",
    "\n",
    "    #if cnt + 1 == len(dupeList):\n",
    "    #Counter\n",
    "    #pprint(filename)\n",
    "    #break\n",
    "#exactDupeDict =\n",
    "    \n",
    "print(f'done, took {(time.time() - crcStart):.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d3c7a4b-9dad-43db-987e-031c13a636b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crcCount['9E997366']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f509b-edb5-4c33-9240-d31a7934515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "li =[x for x in range(10)]\n",
    "for cnt,x in enumerate(li):\n",
    "\tprint(cnt,x, len(li))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ce2e9-426c-42a6-adf8-4b4b5582f8df",
   "metadata": {},
   "source": [
    "### let's do some analysis by file suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6fe374d3-1bd5-4762-8773-dd183eb4d8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.jpg', 6486),\n",
      " ('.py', 3490),\n",
      " ('.pyc', 3278),\n",
      " ('.h', 3213),\n",
      " ('.xml', 3174),\n",
      " ('.png', 2739),\n",
      " ('', 2233),\n",
      " ('.html', 1847),\n",
      " ('.pde', 1780),\n",
      " ('.mkv', 1495),\n",
      " ('.php', 1412),\n",
      " ('.res', 1348),\n",
      " ('.str', 1312),\n",
      " ('.gif', 1109),\n",
      " ('.class', 802),\n",
      " ('.mp3', 668),\n",
      " ('.txt', 569),\n",
      " ('.frm', 514),\n",
      " ('.epub', 504),\n",
      " ('.doc', 493)]\n"
     ]
    }
   ],
   "source": [
    "suffixesCounter = Counter()\n",
    "\n",
    "for k,dupeList in dupeDict.items(): \n",
    "    for file in dupeList:\n",
    "        #for suffix in file['pathObj'].suffixes:\n",
    "        suffixesCounter[file['pathObj'].suffix] += 1\n",
    "suffixesCounter = [(k,v) for k,v in suffixesCounter.items()]\n",
    "suffixesCounter.sort(key = lambda k: k[1], reverse=True)\n",
    "\n",
    "#top 20 duplicated \n",
    "pprint(suffixesCounter[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbff30-ad4d-4402-898e-4db707af568a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### start by figuring out duplication categories\n",
    " - same name, same size, same mtime, same hash different dedupeDirs\n",
    "     - can safely delete one of them based on priority\n",
    " - same name, same size, different mtime\n",
    "     - hash and branch logic if different\n",
    " - same name, different size \n",
    "     - send to manual review queue, potentially prefer the newer one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8c651f7-4833-464b-91af-3093b951e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'mtime': 1607128670.8325891,\n",
      "  'path': 'C:/Users/Ryan/Desktop/desktop.ini',\n",
      "  'pathObj': WindowsPath('C:/Users/Ryan/Desktop/desktop.ini'),\n",
      "  'priority': 0,\n",
      "  'rootTier': 0,\n",
      "  'size': 282},\n",
      " {'mtime': 1538450652.6097116,\n",
      "  'path': 'C:/Users/Ryan/Desktop/Files/Home and Money/Tax/2015 Tax/desktop.ini',\n",
      "  'pathObj': WindowsPath('C:/Users/Ryan/Desktop/Files/Home and Money/Tax/2015 Tax/desktop.ini'),\n",
      "  'priority': 1,\n",
      "  'rootTier': 0,\n",
      "  'size': 136},\n",
      " {'mtime': 1577863147.0110621,\n",
      "  'path': 'C:/Users/Ryan/Desktop/Files/Music/desktop.ini',\n",
      "  'pathObj': WindowsPath('C:/Users/Ryan/Desktop/Files/Music/desktop.ini'),\n",
      "  'priority': 2,\n",
      "  'rootTier': 0,\n",
      "  'size': 440},\n",
      " {'mtime': 1538450652.605711,\n",
      "  'path': 'C:/Users/Ryan/Desktop/Files/photos/Old Photos/Trivia '\n",
      "          'Pics/desktop.ini',\n",
      "  'pathObj': WindowsPath('C:/Users/Ryan/Desktop/Files/photos/Old Photos/Trivia Pics/desktop.ini'),\n",
      "  'priority': 3,\n",
      "  'rootTier': 0,\n",
      "  'size': 136},\n",
      " {'mtime': 1616551386.9991388,\n",
      "  'path': 'C:/Users/Ryan/Desktop/Files/Work/work git/SQL/desktop.ini',\n",
      "  'pathObj': WindowsPath('C:/Users/Ryan/Desktop/Files/Work/work git/SQL/desktop.ini'),\n",
      "  'priority': 4,\n",
      "  'rootTier': 0,\n",
      "  'size': 110},\n",
      " {'mtime': 1538450652.6097116,\n",
      "  'path': 'F:/Files/Home and Money/Tax/2015 Tax/desktop.ini',\n",
      "  'pathObj': WindowsPath('F:/Files/Home and Money/Tax/2015 Tax/desktop.ini'),\n",
      "  'priority': 5,\n",
      "  'rootTier': 2,\n",
      "  'size': 136},\n",
      " {'mtime': 1577863147.0110621,\n",
      "  'path': 'F:/Files/Music/desktop.ini',\n",
      "  'pathObj': WindowsPath('F:/Files/Music/desktop.ini'),\n",
      "  'priority': 6,\n",
      "  'rootTier': 2,\n",
      "  'size': 440},\n",
      " {'mtime': 1538450652.605711,\n",
      "  'path': 'F:/Files/photos/Old Photos/Trivia Pics/desktop.ini',\n",
      "  'pathObj': WindowsPath('F:/Files/photos/Old Photos/Trivia Pics/desktop.ini'),\n",
      "  'priority': 7,\n",
      "  'rootTier': 2,\n",
      "  'size': 136},\n",
      " {'mtime': 1579135968.026018,\n",
      "  'path': 'F:/Oculus Apps/desktop.ini',\n",
      "  'pathObj': WindowsPath('F:/Oculus Apps/desktop.ini'),\n",
      "  'priority': 8,\n",
      "  'rootTier': 2,\n",
      "  'size': 127}]\n"
     ]
    }
   ],
   "source": [
    "if type(dedupeDirs[0]) == str:\n",
    "    dedupeDirs = [(rootTier,Path(directory).as_posix()) for rootTier,directory in enumerate(dedupeDirs)]\n",
    "\n",
    "for k,dupeList in dupeDict.items(): \n",
    "    for cnt, file in enumerate(dupeList):\n",
    "        #assign a tier to each identified dupe based on the directory order given in the first user parameters\n",
    "        for rootTier, dedupeDir in dedupeDirs:\n",
    "            if file['path'].startswith(dedupeDir):\n",
    "                file['rootTier'] = rootTier\n",
    "    dupeList = sorted(dupeList, key=lambda k: (k['rootTier'], len('path')))\n",
    "    for cnt,file in enumerate(dupeList):\n",
    "        file['priority'] = cnt \n",
    "    pprint(dupeList)\n",
    "\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1afc1-c159-407a-9df9-dd68a9787ad9",
   "metadata": {},
   "source": [
    "### Let's get deleting files\n",
    "Work out the priority of each file to conslidate everything on one disk first.\n",
    "\n",
    "Dry run is enabled unless you disable it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fefaee5f-f53f-4614-84cd-5f51fecef86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bd04f41-3251-475e-b016-0a814eaedfda",
   "metadata": {},
   "source": [
    "dryRun = True\n",
    "\n",
    "## TODO:\n",
    " - compare by file attributes, basically just size and name\n",
    "     -  then by hash if same name/size\n",
    " - if name/size disagree, put in a conflict csv to review later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e38b1-d315-45a5-96ff-d36bd1a0975b",
   "metadata": {},
   "source": [
    "\n",
    "Bold \t**bold text**\n",
    "*italicized text*\n",
    "\n",
    "Blockquote \t> blockquote\n",
    "Ordered List \t\n",
    "1. First item\n",
    "2. Second item\n",
    "3. Third item\n",
    "Unordered List \t\n",
    "- First item\n",
    "- Second item\n",
    "- Third item\n",
    "Code \t`code`\n",
    "Horizontal Rule \t---\n",
    "Link \t\n",
    "[title](https://www.example.com)\n",
    "Image \t\n",
    "![alt text](image.jpg)\n",
    "\n",
    "### Results \n",
    "| Stretch/Untouched | ProbDistribution | Accuracy |\n",
    "| :- | -: | :-: |\n",
    "| Stretched | Gaussian | .843\n",
    "\n",
    " ```\n",
    "{\n",
    "  \"firstName\": \"John\",\n",
    "  \"lastName\": \"Smith\",\n",
    "  \"age\": 25\n",
    "}\n",
    "``` \n",
    "footnote \tHere's a sentence with a footnote. [^1]\n",
    "\n",
    "[^1]: This is the footnote.\n",
    "Heading ID \n",
    "### My Great Heading {#custom-id}\n",
    "Definition List \t\n",
    "term\n",
    ": definition\n",
    "\n",
    "Task List \t\n",
    "- [x] Write the press release\n",
    "- [ ] Update the website\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
