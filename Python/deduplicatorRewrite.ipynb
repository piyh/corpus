{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b96f57-6de8-42ad-b2e2-3fac25001bb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# THE GREAT FILE DE-DUPLICATOR\n",
    "### A solution to my fractured backups\n",
    "A program of 1000 lines begins with dependency imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a3dd947-fe45-4994-b5a2-da49210349c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from multiprocess import Pool, cpu_count\n",
    "from pprint import pprint, pformat\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output, display, HTML\n",
    "import sys\n",
    "import time\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97978f-baa3-4efe-9cd3-9021e6a3e295",
   "metadata": {},
   "source": [
    "### Enter your parameters\n",
    "Enter where you want to output your analysis files, what directories you want to analyze for duplicates and which folder names you wish to exclude from the analysis.\n",
    "Mind the backslashes playing nice with python on windows.\n",
    "\n",
    "**THE ORDER THE deDupeDirs ARE IN ARE THE PREFERRED ORDER FOR RETENTION IF DRY RUN IS DISABLED! MAKE SURE YOU ARE OK WITH LATER ENTRIES IN THE LIST HAVING DUPLICATE FILES DISABLED!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e952a8d5-8bd0-48aa-9ffd-f59bb1353e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C:\\\\Users\\\\Ryan\\\\Desktop', 'D:\\\\Users\\\\Ryan', 'G:', 'F:')\n"
     ]
    }
   ],
   "source": [
    "dedupeDirs = (r'C:\\Users\\Ryan\\Desktop',\n",
    "               r'D:\\Users\\Ryan',\n",
    "               'G:',\n",
    "               'F:',\n",
    "               #'H:\\\\',\n",
    "               )\n",
    "excludeDirs = {'$RECYCLE.BIN',\n",
    "               'gdpr data',\n",
    "               'steamapps',\n",
    "               'Google Dump',\n",
    "               'Windows',\n",
    "              }\n",
    "excludeExtensions = {'.example',    \n",
    "}\n",
    "\n",
    "outputDir = Path(r'C:\\Users\\Ryan\\Desktop')\n",
    "\n",
    "#Assign file retention priority to dedupeDirs based on order\n",
    "#Early entries in the list will keep their files over later entries with duplicate files\n",
    "print(dedupeDirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e79f80-50d1-4bc7-894d-655c624f0447",
   "metadata": {},
   "source": [
    "## Index all files in directories\n",
    "For a directories listed in \"deDupeDirs\", get the filenames, last modified dates, and file size. \n",
    "End result is a dataframe with all files listed within those dirs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658c21d6-1711-4f63-b196-f7084dd4c9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running in C:\\Users\\Ryan\\Desktop\n",
      "len(files)=56923\n",
      "len(errors)=0\n",
      "running in D:\\Users\\Ryan\n",
      "len(files)=208187\n",
      "len(errors)=9\n",
      "running in G:\n",
      "len(files)=231981\n",
      "len(errors)=0\n",
      "running in F:\n",
      "len(files)=252510\n",
      "len(errors)=0\n",
      "loading dataframe\n",
      "dupeDataframe loaded\n",
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dedupePaths = [Path(x) for x in dedupeDirs]\n",
    "\n",
    "files = []\n",
    "for workDir in dedupePaths:\n",
    "    errors = []\n",
    "    print(f'running in {workDir}')\n",
    "    for file in workDir.glob('**/*'):\n",
    "        try:\n",
    "            if (   file.is_file() \n",
    "                    and not excludeDirs.intersection(set(file.parts)) \n",
    "                    and not excludeExtensions.intersection(set(file.suffixes))\n",
    "               ):\n",
    "                fileDict = {'path':file,\n",
    "                            #'path':file.as_posix(),\n",
    "                            'name':file.name,\n",
    "                            'size':file.stat().st_size,\n",
    "                            'mtime':file.stat().st_mtime,\n",
    "                           }\n",
    "                files.append(fileDict)\n",
    "        except:\n",
    "                errors.append(file)\n",
    "    print(f'{len(files)=}')\n",
    "    print(f'{len(errors)=}')\n",
    "\n",
    "print('loading dataframe')\n",
    "df = pd.DataFrame(files)\n",
    "print('dupeDataframe loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e3d85-44b9-46be-9899-c6d5619f8bd9",
   "metadata": {},
   "source": [
    "### Save your progress\n",
    "The above steps could take a while depending on how much you've hoarded, save the output to a JSON file to noodle with at a later date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed87fd8-098b-47f3-90d3-2c6122b02535",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickled and csved df\n",
      "Wall time: 1.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.to_pickle(r'C:\\Users\\Ryan\\Desktop\\filelist.pkl')\n",
    "df.to_csv('filelist.csv', index = False)\n",
    "print('pickled and csved df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ef5e7-b5a4-4932-b8cf-17c4d9f3f553",
   "metadata": {},
   "source": [
    "### Reload your progress\n",
    "Reload if you saved to a JSON file.  You'll need to run imports and the parameter code blocks before this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b393ad2-617a-4d08-96af-521165b582de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_pickle(r'C:\\Users\\Ryan\\Desktop\\filelist.pkl')                       \n",
    "print('loaded from file')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e154ab5-5672-4ec6-b434-edb872a2b390",
   "metadata": {},
   "source": [
    "### Define functions used in duplicate processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41860f9d-2e72-4c5a-ba70-e3e3138bf25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def mpCrc32(fileName):\n",
    "    #have to import within function because of multiprocessing\n",
    "    from zlib import crc32\n",
    "    try:\n",
    "        with open(fileName, 'rb') as fh:\n",
    "            hash = 0\n",
    "            while True:\n",
    "                s = fh.read(65536)\n",
    "                if not s:\n",
    "                    break\n",
    "                hash = crc32(s, hash)\n",
    "            return \"%08X\" % (hash & 0xFFFFFFFF)\n",
    "    except Exception as e:\n",
    "        x = f'{e}, {fileName}'\n",
    "        return x       \n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d1a16-dc51-47ee-8527-5ac63909fd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2%"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#shuffle your dataframe to pull semi randomly and so we don't linearly iterate over folders\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "crcList = []\n",
    "with Pool(processes = int(cpu_count()/2), maxtasksperchild = 500) as p:\n",
    "    for cnt, crc in enumerate(p.imap(func = mpCrc32, iterable = df['path'], chunksize = 5000)):\n",
    "        crcList.append(crc)\n",
    "        sys.stderr.write(f'\\r{cnt/len(df):.0%}')\n",
    "df['crc'] = crcList\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fa6ca-61b2-496f-a29d-4f8067663b59",
   "metadata": {},
   "source": [
    "## Checkpoint your progress  after calculating CRCs\n",
    "It takes a while to get this far since it's basically a full disk read, save your progress!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8656a90-36fd-483a-b2ae-c8b9b7530b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_pickle(r'C:\\Users\\Ryan\\Desktop\\filelistCRC.pkl')\n",
    "df.to_csv('filelistCRC.csv', index = False)\n",
    "print('pickled CRC populated dataframe and csved df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b764d-0385-4faa-8080-3e26d7e95707",
   "metadata": {},
   "source": [
    "### Reload progress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8805a-4e8a-4864-9e8a-e6617984a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_pickle(r'C:\\Users\\Ryan\\Desktop\\filelistCRC.pkl')                       \n",
    "print('loaded from file')\n",
    "print(f'{len(df)=}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0beb0570-b5a2-4001-bc8e-e08394039132",
   "metadata": {},
   "source": [
    "TODO:\n",
    "group by a combination of name, size, crc.\n",
    "if the crc checks match, analyze \n",
    "group by analysis has an order by clause, keep the top x and have a function for delete with dry run as the base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7047c56-70a0-4d3c-a34e-5891750a4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dedupeTiers = [(rootTier,Path(directory).as_posix()) for rootTier,directory in enumerate(dedupeDirs)]\n",
    "print(dedupeTiers)\n",
    "def getTier(file):\n",
    "    file = file.as_posix()\n",
    "    return int([rootTier for rootTier, posix_path in dedupeTiers if file.startswith(posix_path)][0])\n",
    "    \n",
    "print(df['path'][0].as_posix())\n",
    "print(getTier(df['path'][0]))\n",
    "df['rootTier'] = df['path'].apply(getTier)\n",
    "\n",
    "#df['rootTier'] = [getTier(x) for x in df['path']]\n",
    "df.dtypes\n",
    "#df['path'].map()\n",
    "#df['sourceTier'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2d571-93ae-4220-bcb5-41af1481d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#sort df before the group by\n",
    "df['pathLen'] = df['path'].map(Path.as_posix).map(len)\n",
    "df.sort_values(by= ['name','rootTier', 'pathLen'],  inplace = True)\n",
    "del df['pathLen']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0db0d8-5ff3-4a78-b3dc-8d95ec61367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#let's try to find the most common folders stuff is in\n",
    "df['parent'] = [x.parent for x in df['path']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925151c9-b542-4eeb-b71a-ee3d4517e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c3cde-ec6e-46d4-a665-a26fac4c8d6f",
   "metadata": {},
   "source": [
    "### Filter dataframe down to duplicate CRCs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566766a7-5b1c-42f0-bc0d-9105c7013671",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grouped = df.groupby('crc')\n",
    "#grouped.filter('')\n",
    "grouped = grouped.count().sort_values('path', ascending = False)\n",
    "grouped = grouped[grouped['path']>1]\n",
    "duplicatedCRCs = list(grouped.index)\n",
    "df = df[df['crc'].isin(duplicatedCRCs)]\n",
    "grouped\n",
    "#grouped.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e196886-14cd-4e70-bfdf-0eba55e0c980",
   "metadata": {},
   "source": [
    "### Only run on exact duplicates first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca46886-2137-4dfc-b23b-53cd0b829086",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%time\n",
    "#my group by root tier logic seems to be working \n",
    "grouped = df.groupby('crc', sort = False)\n",
    "\n",
    "for cnt, (crc, group) in enumerate(grouped):\n",
    "    if len(group) == 1 or crc == '00000000':\n",
    "        continue\n",
    "    if group['rootTier'].min() == group['rootTier'].max():\n",
    "        continue\n",
    "    clear_output()\n",
    "    display(group[group['rootTier'] == group['rootTier'].max()])\n",
    "    display(group)\n",
    "    time.sleep(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a96c284-3916-4808-9ec7-57f0784c6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupeList = sorted(dupeList, key=lambda k: (k['rootTier'], len('path')))\n",
    "for cnt,file in enumerate(dupeList):\n",
    "    file['priority'] = cnt \n",
    "pprint(dupeList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ce2e9-426c-42a6-adf8-4b4b5582f8df",
   "metadata": {},
   "source": [
    "### let's do some analysis by file suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe374d3-1bd5-4762-8773-dd183eb4d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixesCounter = Counter()\n",
    "\n",
    "for k,dupeList in dupeDict.items(): \n",
    "    for file in dupeList:\n",
    "        #for suffix in file['pathObj'].suffixes:\n",
    "        suffixesCounter[file['pathObj'].suffix] += 1\n",
    "suffixesCounter = [(k,v) for k,v in suffixesCounter.items()]\n",
    "suffixesCounter.sort(key = lambda k: k[1], reverse=True)\n",
    "\n",
    "#top 20 duplicated \n",
    "pprint(suffixesCounter[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbff30-ad4d-4402-898e-4db707af568a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### start by figuring out duplication categories\n",
    " - same name, same size, same mtime, same hash different dedupeDirs\n",
    "     - can safely delete one of them based on priority\n",
    " - same name, same size, different mtime\n",
    "     - hash and branch logic if different\n",
    " - same name, different size \n",
    "     - send to manual review queue, potentially prefer the newer one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78968684-0a99-4fa3-aac4-25038435592d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-11aecd33e5a8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-11aecd33e5a8>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    For my deletion logic I need the dry run I need to output a file that would say here's the original path and here's the moved to path which is basically the recycle bin and then I need to be able to restore files to and from that recycle bin before permanent deletion\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "For my deletion logic I need the dry run I need to output a file that would say here's the original path and here's the moved to path which is basically the recycle bin and then I need to be able to restore files to and from that recycle bin before permanent deletion\n",
    "\n",
    "Make sure this code to restore stuff from the recycle bin as well tested so that I don't break any processes I can't remember\n",
    "\n",
    "All unique files on my non-primary drive need to be pulled over and brought into the formal file structure\n",
    "\n",
    "Look into whatever line is tech tips use that was basically the r seeing log rotate thing where I can automate my backups I need to have snapshots as well as like I don't schedule to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1afc1-c159-407a-9df9-dd68a9787ad9",
   "metadata": {},
   "source": [
    "### Let's get deleting files\n",
    "Work out the priority of each file to conslidate everything on one disk first.\n",
    "\n",
    "Dry run is enabled unless you disable it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e416d8-c2b3-4cd1-a6a5-9e5a68a51c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return f\"{num:3.1f}{unit}{suffix}\" \n",
    "        num /= 1024.0\n",
    "    return \"{num:.1f}Yi{suffix}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7850d7c8-fc98-405a-81e2-fdaed8824d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Assuming that dataframes df1 and df2 are already defined:\n",
    "\n",
    "#display(df)\n",
    "display(df.to_html())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd04f41-3251-475e-b016-0a814eaedfda",
   "metadata": {},
   "source": [
    "dryRun = True\n",
    "\n",
    "## TODO:\n",
    " - compare by file attributes, basically just size and name\n",
    "     -  then by hash if same name/size\n",
    " - if name/size disagree, put in a conflict csv to review later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e38b1-d315-45a5-96ff-d36bd1a0975b",
   "metadata": {},
   "source": [
    "\n",
    "Bold \t**bold text**\n",
    "*italicized text*\n",
    "\n",
    "Blockquote \t> blockquote\n",
    "Ordered List \t\n",
    "1. First item\n",
    "2. Second item\n",
    "3. Third item\n",
    "Unordered List \t\n",
    "- First item\n",
    "- Second item\n",
    "- Third item\n",
    "Code \t`code`\n",
    "Horizontal Rule \t---\n",
    "Link \t\n",
    "[title](https://www.example.com)\n",
    "Image \t\n",
    "![alt text](image.jpg)\n",
    "\n",
    "### Results \n",
    "| Stretch/Untouched | ProbDistribution | Accuracy |\n",
    "| :- | -: | :-: |\n",
    "| Stretched | Gaussian | .843\n",
    "\n",
    " ```\n",
    "{\n",
    "  \"firstName\": \"John\",\n",
    "  \"lastName\": \"Smith\",\n",
    "  \"age\": 25\n",
    "}\n",
    "``` \n",
    "footnote \tHere's a sentence with a footnote. [^1]\n",
    "\n",
    "[^1]: This is the footnote.\n",
    "Heading ID \n",
    "### My Great Heading {#custom-id}\n",
    "Definition List \t\n",
    "term\n",
    ": definition\n",
    "\n",
    "Task List \t\n",
    "- [x] Write the press release\n",
    "- [ ] Update the website\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
