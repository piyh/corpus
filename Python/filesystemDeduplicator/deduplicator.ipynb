{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b96f57-6de8-42ad-b2e2-3fac25001bb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# THE GREAT FILE DE-DUPLICATOR\n",
    "### A solution to my fractured backups\n",
    "A program of 1000 lines begins with dependency imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a3dd947-fe45-4994-b5a2-da49210349c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import multiprocess #multiprocessing if running outside of jupyter\n",
    "from multiprocess import Process, Queue #multiprocessing if running outside of jupyter\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pprint import pprint, pformat\n",
    "import time\n",
    "import logging\n",
    "from queue import Empty\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#import pandas as pd\n",
    "#import zlib\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97978f-baa3-4efe-9cd3-9021e6a3e295",
   "metadata": {},
   "source": [
    "### Enter your parameters\n",
    "Enter where you want to output your analysis files, what directories you want to analyze for duplicates and which folder names you wish to exclude from the analysis.\n",
    "Mind the backslashes playing nice with python on windows.\n",
    "\n",
    "**THE ORDER THE deDupeDirs ARE IN ARE THE PREFERRED ORDER FOR RETENTION IF DRY RUN IS DISABLED! MAKE SURE YOU ARE OK WITH LATER ENTRIES IN THE LIST HAVING DUPLICATE FILES DISABLED!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e952a8d5-8bd0-48aa-9ffd-f59bb1353e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C:\\\\Users\\\\Ryan\\\\Desktop', 'D:\\\\Users\\\\Ryan', 'G:', 'F:')\n"
     ]
    }
   ],
   "source": [
    "dedupeDirs = (r'C:\\Users\\Ryan\\Desktop',\n",
    "               r'D:\\Users\\Ryan',\n",
    "               'G:',\n",
    "               'F:',\n",
    "               #'H:\\\\',\n",
    "               )\n",
    "excludeDirs = {'$RECYCLE.BIN',\n",
    "               'gdpr data',\n",
    "               'steamapps',\n",
    "               'Google Dump',\n",
    "               'Windows',\n",
    "              }\n",
    "excludeExtensions = {'.example',    \n",
    "}\n",
    "outputDir = Path(r'C:\\Users\\Ryan\\Desktop')\n",
    "\n",
    "#Assign file retention priority to dedupeDirs based on order\n",
    "#Early entries in the list will keep their files over later entries with duplicate files\n",
    "print(dedupeDirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e79f80-50d1-4bc7-894d-655c624f0447",
   "metadata": {},
   "source": [
    "## Find duplicates based on file name\n",
    "For a given directory listed in \"deDupeDirs\", get the filenames, last modified dates, and file size. \n",
    "End result of a list will be a dict with filename as keys and values that are lists of dicts of file paths and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "658c21d6-1711-4f63-b196-f7084dd4c9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running in C:\\Users\\Ryan\\Desktop\n",
      "len of files parsed =  41595\n",
      "len(errors)=0\n",
      "processed C:\\Users\\Ryan\\Desktop in 20.11954116821289s\n",
      "running in D:\\Users\\Ryan\n",
      "len of files parsed =  127057\n",
      "len(errors)=9\n",
      "processed D:\\Users\\Ryan in 42.75087833404541s\n",
      "running in G:\n",
      "len of files parsed =  127057\n",
      "len(errors)=0\n",
      "processed G: in 0.0s\n",
      "running in F:\n",
      "len of files parsed =  132049\n",
      "len(errors)=0\n",
      "processed F: in 54.5726056098938s\n",
      "number of uniques =  95809\n",
      "number of dupes =  36240\n",
      "dict load completed in 117.52304244041443s\n",
      "loading dataframe\n",
      "dupeDataframe loaded\n",
      "df load completed in 118.02515625953674s\n"
     ]
    }
   ],
   "source": [
    "dupeSearchStart = time.time()\n",
    "dedupePaths = [Path(x) for x in dedupeDirs]\n",
    "\n",
    "files = []\n",
    "dupeDict = defaultdict(list)\n",
    "for workDir in dedupePaths:\n",
    "    errors = []\n",
    "    print(f'running in {workDir}')\n",
    "    dirDupeSearchStart = time.time()\n",
    "    for file in workDir.glob('**/*'):\n",
    "        try:\n",
    "            if file.is_file() and not excludeDirs.intersection(set(file.parts)) and not excludeExtensions.intersection(set(file.suffixes)):\n",
    "                fileDict = {'pathObj':file,\n",
    "                            'path':file.as_posix(),\n",
    "                            'name':file.name,\n",
    "                            'size':file.stat().st_size,\n",
    "                            'mtime':file.stat().st_mtime,\n",
    "                            }\n",
    "                dupeDict[file.name].append(fileDict)\n",
    "                files.append(fileDict)\n",
    "        except:\n",
    "                errors.append(file)\n",
    "            #    raise\n",
    "    print('len of files parsed = ', len(dupeDict))\n",
    "    print(f'{len(errors)=}')\n",
    "\n",
    "    print(f'processed {workDir} in {time.time() - dirDupeSearchStart}s')\n",
    "\n",
    "uniqueDict = {k:v for k,v in dupeDict.items() if len(v) == 1}\n",
    "print('number of uniques = ', len(uniqueDict))\n",
    "dupeDict   = {k:v for k,v in dupeDict.items() if len(v) > 1}\n",
    "print('number of dupes = ', len(dupeDict))\n",
    "print(f'dict load completed in {time.time() - dupeSearchStart}s')\n",
    "print('loading dataframe')\n",
    "df = pd.DataFrame(files)\n",
    "print('dupeDataframe loaded')\n",
    "print(f'df load completed in {time.time() - dupeSearchStart}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e3d85-44b9-46be-9899-c6d5619f8bd9",
   "metadata": {},
   "source": [
    "### Save your progress\n",
    "The above steps could take a while depending on how much you've hoarded, save the output to a JSON file to noodle with at a later date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ed87fd8-098b-47f3-90d3-2c6122b02535",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to file\n",
      "len(errors)=228677\n",
      "successCount=0\n",
      "done, took 3.38s\n",
      "pickled df\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "errors = []\n",
    "successCount = 0\n",
    "for d in dicts:\n",
    "    for k,li in d.items():\n",
    "        for file in li:\n",
    "            try:\n",
    "                del file['pathObj']\n",
    "                successCount += 1\n",
    "            except:\n",
    "                errors.append(file)\n",
    "                \n",
    "                #raise\n",
    "print(f'{len(errors)=}')\n",
    "print(f'{successCount=}')\n",
    "                \n",
    "with open(outputDir.joinpath('duplicateFiles.json'),'w',encoding='utf8') as f:\n",
    "    json.dump(dupeDict, f, indent = 1 )\n",
    "    \n",
    "with open(outputDir.joinpath('uniqueFiles.json'),'w',encoding='utf8') as f:\n",
    "    json.dump(uniqueDict, f, indent = 1 )\n",
    "print(f'done, took {time.time() - writingStart:.2f}s')\n",
    "\n",
    "df.to_pickle(r'C:\\Users\\Ryan\\Desktop\\filelist.pkl')\n",
    "print('pickled df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e8bc87f-0381-4018-8846-9367ae3b64ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dupeDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cbbd89761ce0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdupeDict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#df.to_csv('out.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dupeDict' is not defined"
     ]
    }
   ],
   "source": [
    "for d in dupeDict.items():\n",
    "    print(d)\n",
    "    break\n",
    "    \n",
    "#df.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49b733c8-611a-4661-8613-8b26f8f698ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import sleep\n",
    "#for x in range(3):\n",
    "sleep(1)\n",
    "x = [x for x in range(19999999)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ef5e7-b5a4-4932-b8cf-17c4d9f3f553",
   "metadata": {},
   "source": [
    "### Reload your progress\n",
    "Reload if you saved to a JSON file.  You'll need to run imports and the parameter code blocks before this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe08d08-a76c-4bf3-a8ed-8779560dfe6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6e686a7a93c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloadStart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Ryan\\Desktop\\filelist.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'done, took {(time.time() - loadStart):.2f}s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "loadStart = time.time()\n",
    "\n",
    "df = pd.read_pickle(r'C:\\Users\\Ryan\\Desktop\\filelist.pkl')            \n",
    "            \n",
    "print(f'done, took {(time.time() - loadStart):.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e154ab5-5672-4ec6-b434-edb872a2b390",
   "metadata": {},
   "source": [
    "### Define functions used in duplicate processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41860f9d-2e72-4c5a-ba70-e3e3138bf25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def mpCrc32(fileName, q, printQueue):\n",
    "    #have to import within function because of multiprocessing\n",
    "    from zlib import crc32\n",
    "    try:\n",
    "        with open(fileName, 'rb') as fh:\n",
    "            hash = 0\n",
    "            while True:\n",
    "                s = fh.read(65536)\n",
    "                if not s:\n",
    "                    break\n",
    "                hash = crc32(s, hash)\n",
    "            returnVal = {'file':Path(fileName),'crc32':\"%08X\" % (hash & 0xFFFFFFFF)}\n",
    "            #printQueue.put(returnVal)\n",
    "            q.put(returnVal)\n",
    "    except Exception as e:\n",
    "        printQueue.put(f'error on {fileName}, {e}')\n",
    "        raise\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return f\"{num:3.1f}{unit}{suffix}\" \n",
    "        num /= 1024.0\n",
    "    return \"{num:.1f}Yi{suffix}\"\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dbdfd7-b540-4eb8-bb64-589db17903e3",
   "metadata": {},
   "source": [
    "ok so I think dataframes are the way to go\n",
    "\n",
    "get a list of all files\n",
    "\n",
    "group by name or size or name then size.\n",
    "\n",
    "run the same crc checks on a group\n",
    "\n",
    "if the crc checks match, analyze \n",
    "\n",
    "group by analysis has an order by clause, keep the top x and have a function for delete with dry run as the base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b193bb-3ac2-4d6e-9d78-9a2791d54847",
   "metadata": {},
   "source": [
    "### Only run on exact duplicates first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca46886-2137-4dfc-b23b-53cd0b829086",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dupeGroupCount=0 of len(dupeDict)=17257 for a % complete of 0.00\n",
      "processing C:/Users/Ryan/Desktop/desktop.ini 282.0B\n",
      "processing C:/Users/Ryan/Desktop/Files/Home and Money/Tax/2015 Tax/desktop.ini 136.0B\n",
      "processing C:/Users/Ryan/Desktop/Files/Music/desktop.ini 440.0B\n",
      "processing C:/Users/Ryan/Desktop/Files/photos/Old Photos/Trivia Pics/desktop.ini 136.0B\n",
      "processing C:/Users/Ryan/Desktop/Files/Work/work git/SQL/desktop.ini 110.0B\n",
      "processing F:/Files/Home and Money/Tax/2015 Tax/desktop.ini 136.0B\n",
      "processing F:/Files/Music/desktop.ini 440.0B\n",
      "breaking for debugging\n",
      "processing C:/Users/Ryan/Desktop/Files/Guide2DataMining.pdf 138.6MiB\n",
      "processing F:/Files/Guide2DataMining.pdf 138.6MiB\n",
      "processing C:/Users/Ryan/Desktop/Files/rsync_backup.sh 43.0B\n",
      "processing F:/Files/rsync_backup.sh 43.0B\n",
      "processing C:/Users/Ryan/Desktop/Files/Setup.x64.en-US_ProPlusRetail_9WJKN-DG84F-FQR4V-BGDXG-72XKQ_TX_PR_act_1_.exe 1.4MiB\n",
      "processing F:/Files/Setup.x64.en-US_ProPlusRetail_9WJKN-DG84F-FQR4V-BGDXG-72XKQ_TX_PR_act_1_.exe 1.4MiB\n",
      "processing C:/Users/Ryan/Desktop/Files/backups/keeper of files 24-aug-2018 backup.tar 49.8GiB\n",
      "processing F:/Files/backups/keeper of files 24-aug-2018 backup.tar 49.8GiB\n",
      "processing C:/Users/Ryan/Desktop/Files/backups/rpi backup 22-Aug-2018.7z 19.3GiB\n",
      "processing F:/Files/backups/rpi backup 22-Aug-2018.7z 19.3GiB\n",
      "processing C:/Users/Ryan/Desktop/Files/backups/spacebucket 31-aug-2018 backup.7z 630.1MiB\n",
      "processing F:/Files/backups/spacebucket 31-aug-2018 backup.7z 630.1MiB\n",
      "breaking for debugging\n",
      "joining processes\n"
     ]
    }
   ],
   "source": [
    "progressTick = 20\n",
    "progressTick = len(dupeDict.values()) % progressTick\n",
    "maxProcesses = multiprocess.cpu_count()/2\n",
    "runningProcesses = []\n",
    "q = Queue()\n",
    "printQ = Queue()\n",
    "crcStart = time.time()\n",
    "\n",
    "print(f'starting {len(dupeDict)=}')\n",
    "\n",
    "for dupeGroupCount, kv in enumerate(dupeDict.items()):\n",
    "    filename,dupeList = kv\n",
    "    \n",
    "    if dupeGroupCount % progressTick == 0 or dupeGroupCount == len(dupeDict) + 1:\n",
    "        clear_output()\n",
    "        print(f'processing {dupeGroupCount=} of {len(dupeDict)=} for a % complete of {(dupeGroupCount/len(dupeDict)):.2f}')\n",
    "\n",
    "    #only things in sizeGroup less than  get a CRC\n",
    "    sizeGroup = defaultdict(list)\n",
    "    for cnt,d in enumerate(dupeList):\n",
    "        sizeGroup[d['size']].append(d['path'])\n",
    "        \n",
    "    #remove all where we don't have a duplicate  filesize\n",
    "    #size less than 1000000000 ties to 1 gig\n",
    "    sizeGroup = {k:v for k,v in sizeGroup.items() if len(v) > 1 and int(k) < 1000000000}\n",
    "    #pprint(sizeGroup)\n",
    "    \n",
    "    for cnt,d in enumerate(dupeList):\n",
    "        #it would be better to have long running processes that get work through the queue instead of 1 process per file \n",
    "        while len(multiprocess.active_children()) == maxProcesses:\n",
    "            time.sleep(.1)\n",
    "\n",
    "        print(f\"processing {d['path']} {sizeof_fmt(d['size'])}\")\n",
    "        process = multiprocess.Process(target=mpCrc32, args =[d['path'],q, printQ])\n",
    "        runningProcesses.append(process)\n",
    "        process.start()\n",
    "            \n",
    "    #time.sleep(1)\n",
    "    \n",
    "        if cnt > 5:\n",
    "            time.sleep(1)\n",
    "            print('breaking for debugging')\n",
    "            break\n",
    "            \n",
    "    if dupeGroupCount > 5:\n",
    "        #time.sleep(1)\n",
    "        print('breaking for debugging')\n",
    "        break\n",
    "        \n",
    "print('joining processes')\n",
    "for p in runningProcesses:\n",
    "    p.join()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        result = q.get(False)  #gotta use False otherwise get is blocking\n",
    "        name      = result['path'].name\n",
    "        posixPath = result['path'].as_posix\n",
    "        found = False\n",
    "        for d in dupeDict[name]:\n",
    "            if d['path'] == posixPath:\n",
    "                found = True\n",
    "                d.update(result)\n",
    "                print(d)\n",
    "                break\n",
    "        #print('q',result)\n",
    "    except Empty:\n",
    "        print('empty')\n",
    "        raise Exception\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0fe79-8500-4143-b80e-d82338699ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "startt = time.time()\n",
    "time.sleep(.1)\n",
    "print(time.time() - startt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640fa761-b0cf-45c5-b21a-f1f6432276f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    input()\n",
    "    process = multiprocessing.Process(target=crc32, args =(d['path']))\n",
    "    runningProcesses.append(process)\n",
    "    process.start()\n",
    "\n",
    "    for p in runningProcesses:\n",
    "        p.join()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def my_func(arg):\n",
    "        Q.put('Hello, ' + arg)\n",
    "\n",
    "    p1 = Process(target=my_func, args=('John',))\n",
    "    p1.start()\n",
    "    print(Q.get())\n",
    "    p1.join()\n",
    "        \n",
    "    #fileSizeMatch = Counter([d['size']])\n",
    "    crcCount = Counter([d['crc32'] for d in dupeList])\n",
    "\n",
    "    for d in dupeList:\n",
    "        d['crcDupeCount'] = crcCount[d['crc32']]\n",
    "    \n",
    "    dupeList.sort(key=lambda x: x['crcDupeCount'], reverse=True)\n",
    "    #pprint(dupeList)\n",
    "\n",
    "    #if cnt + 1 == len(dupeList):\n",
    "    #Counter\n",
    "    #pprint(filename)\n",
    "    #break\n",
    "#exactDupeDict =\n",
    "    \n",
    "print(f'done, took {(time.time() - crcStart):.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d3c7a4b-9dad-43db-987e-031c13a636b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crcCount['9E997366']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f509b-edb5-4c33-9240-d31a7934515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "li =[x for x in range(10)]\n",
    "for cnt,x in enumerate(li):\n",
    "\tprint(cnt,x, len(li))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ce2e9-426c-42a6-adf8-4b4b5582f8df",
   "metadata": {},
   "source": [
    "### let's do some analysis by file suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe374d3-1bd5-4762-8773-dd183eb4d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixesCounter = Counter()\n",
    "\n",
    "for k,dupeList in dupeDict.items(): \n",
    "    for file in dupeList:\n",
    "        #for suffix in file['pathObj'].suffixes:\n",
    "        suffixesCounter[file['pathObj'].suffix] += 1\n",
    "suffixesCounter = [(k,v) for k,v in suffixesCounter.items()]\n",
    "suffixesCounter.sort(key = lambda k: k[1], reverse=True)\n",
    "\n",
    "#top 20 duplicated \n",
    "pprint(suffixesCounter[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbff30-ad4d-4402-898e-4db707af568a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### start by figuring out duplication categories\n",
    " - same name, same size, same mtime, same hash different dedupeDirs\n",
    "     - can safely delete one of them based on priority\n",
    " - same name, same size, different mtime\n",
    "     - hash and branch logic if different\n",
    " - same name, different size \n",
    "     - send to manual review queue, potentially prefer the newer one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8c651f7-4833-464b-91af-3093b951e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'mtime': 1607128670.8325891,\n",
      "  'path': 'C:/Users/Ryan/Desktop/desktop.ini',\n",
      "  'pathObj': WindowsPath('C:/Users/Ryan/Desktop/desktop.ini'),\n",
      "  'priority': 0,\n",
      "  'rootTier': 0,\n",
      "  'size': 282},\n",
      " {'mtime': 1538450652.6097116,\n",
      "  'path': 'C:/Users/Ryan/Desktop/Files/Home and Money/Tax/2015 Tax/desktop.ini',\n",
      "  'pathObj': WindowsPath('C:/Users/Ryan/Desktop/Files/Home and Money/Tax/2015 Tax/desktop.ini'),\n",
      "  'priority': 1,\n",
      "  'rootTier': 0,\n",
      "  'size': 136},\n",
      " {'mtime': 1577863147.0110621,\n",
      "  'path': 'C:/Users/Ryan/Desktop/Files/Music/desktop.ini',\n",
      "  'pathObj': WindowsPath('C:/Users/Ryan/Desktop/Files/Music/desktop.ini'),\n",
      "  'priority': 2,\n",
      "  'rootTier': 0,\n",
      "  'size': 440},\n",
      " {'mtime': 1538450652.605711,\n",
      "  'path': 'C:/Users/Ryan/Desktop/Files/photos/Old Photos/Trivia '\n",
      "          'Pics/desktop.ini',\n",
      "  'pathObj': WindowsPath('C:/Users/Ryan/Desktop/Files/photos/Old Photos/Trivia Pics/desktop.ini'),\n",
      "  'priority': 3,\n",
      "  'rootTier': 0,\n",
      "  'size': 136},\n",
      " {'mtime': 1616551386.9991388,\n",
      "  'path': 'C:/Users/Ryan/Desktop/Files/Work/work git/SQL/desktop.ini',\n",
      "  'pathObj': WindowsPath('C:/Users/Ryan/Desktop/Files/Work/work git/SQL/desktop.ini'),\n",
      "  'priority': 4,\n",
      "  'rootTier': 0,\n",
      "  'size': 110},\n",
      " {'mtime': 1538450652.6097116,\n",
      "  'path': 'F:/Files/Home and Money/Tax/2015 Tax/desktop.ini',\n",
      "  'pathObj': WindowsPath('F:/Files/Home and Money/Tax/2015 Tax/desktop.ini'),\n",
      "  'priority': 5,\n",
      "  'rootTier': 2,\n",
      "  'size': 136},\n",
      " {'mtime': 1577863147.0110621,\n",
      "  'path': 'F:/Files/Music/desktop.ini',\n",
      "  'pathObj': WindowsPath('F:/Files/Music/desktop.ini'),\n",
      "  'priority': 6,\n",
      "  'rootTier': 2,\n",
      "  'size': 440},\n",
      " {'mtime': 1538450652.605711,\n",
      "  'path': 'F:/Files/photos/Old Photos/Trivia Pics/desktop.ini',\n",
      "  'pathObj': WindowsPath('F:/Files/photos/Old Photos/Trivia Pics/desktop.ini'),\n",
      "  'priority': 7,\n",
      "  'rootTier': 2,\n",
      "  'size': 136},\n",
      " {'mtime': 1579135968.026018,\n",
      "  'path': 'F:/Oculus Apps/desktop.ini',\n",
      "  'pathObj': WindowsPath('F:/Oculus Apps/desktop.ini'),\n",
      "  'priority': 8,\n",
      "  'rootTier': 2,\n",
      "  'size': 127}]\n"
     ]
    }
   ],
   "source": [
    "if type(dedupeDirs[0]) == str:\n",
    "    dedupeDirs = [(rootTier,Path(directory).as_posix()) for rootTier,directory in enumerate(dedupeDirs)]\n",
    "\n",
    "for k,dupeList in dupeDict.items(): \n",
    "    for cnt, file in enumerate(dupeList):\n",
    "        #assign a tier to each identified dupe based on the directory order given in the first user parameters\n",
    "        for rootTier, dedupeDir in dedupeDirs:\n",
    "            if file['path'].startswith(dedupeDir):\n",
    "                file['rootTier'] = rootTier\n",
    "    dupeList = sorted(dupeList, key=lambda k: (k['rootTier'], len('path')))\n",
    "    for cnt,file in enumerate(dupeList):\n",
    "        file['priority'] = cnt \n",
    "    pprint(dupeList)\n",
    "\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1afc1-c159-407a-9df9-dd68a9787ad9",
   "metadata": {},
   "source": [
    "### Let's get deleting files\n",
    "Work out the priority of each file to conslidate everything on one disk first.\n",
    "\n",
    "Dry run is enabled unless you disable it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefaee5f-f53f-4614-84cd-5f51fecef86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess\n",
    "\n",
    "def double(a):\n",
    "    return a * 2\n",
    "\n",
    "def driver_func():\n",
    "    PROCESSES = 4\n",
    "    with multiprocess.Pool(PROCESSES) as pool:\n",
    "        params = [(1, ), (2, ), (3, ), (4, )]\n",
    "        results = [pool.apply_async(double, p) for p in params]\n",
    "\n",
    "        for r in results:\n",
    "            print('\\t', r.get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb15e010-e34b-4377-b240-7c2c5e32f28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 2\n",
      "\t 4\n",
      "\t 6\n",
      "\t 8\n"
     ]
    }
   ],
   "source": [
    "driver_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ceee424-55c4-45a5-8225-8ca94dc6b979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "from multiprocess import Process, Queue\n",
    "\n",
    "def f(q):\n",
    "    q.put('hello world')\n",
    "\n",
    "q = Queue()\n",
    "p = Process(target=f, args=[q])\n",
    "p.start()\n",
    "print (q.get())\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd04f41-3251-475e-b016-0a814eaedfda",
   "metadata": {},
   "source": [
    "dryRun = True\n",
    "\n",
    "## TODO:\n",
    " - compare by file attributes, basically just size and name\n",
    "     -  then by hash if same name/size\n",
    " - if name/size disagree, put in a conflict csv to review later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e38b1-d315-45a5-96ff-d36bd1a0975b",
   "metadata": {},
   "source": [
    "\n",
    "Bold \t**bold text**\n",
    "*italicized text*\n",
    "\n",
    "Blockquote \t> blockquote\n",
    "Ordered List \t\n",
    "1. First item\n",
    "2. Second item\n",
    "3. Third item\n",
    "Unordered List \t\n",
    "- First item\n",
    "- Second item\n",
    "- Third item\n",
    "Code \t`code`\n",
    "Horizontal Rule \t---\n",
    "Link \t\n",
    "[title](https://www.example.com)\n",
    "Image \t\n",
    "![alt text](image.jpg)\n",
    "\n",
    "### Results \n",
    "| Stretch/Untouched | ProbDistribution | Accuracy |\n",
    "| :- | -: | :-: |\n",
    "| Stretched | Gaussian | .843\n",
    "\n",
    " ```\n",
    "{\n",
    "  \"firstName\": \"John\",\n",
    "  \"lastName\": \"Smith\",\n",
    "  \"age\": 25\n",
    "}\n",
    "``` \n",
    "footnote \tHere's a sentence with a footnote. [^1]\n",
    "\n",
    "[^1]: This is the footnote.\n",
    "Heading ID \n",
    "### My Great Heading {#custom-id}\n",
    "Definition List \t\n",
    "term\n",
    ": definition\n",
    "\n",
    "Task List \t\n",
    "- [x] Write the press release\n",
    "- [ ] Update the website\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
